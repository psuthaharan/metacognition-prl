{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load OpenAI API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found.\")\n",
    "client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Define the five metacognitive prompting (MP) dimensions\n",
    "MP_DIMENSIONS = ['Comprehension', 'Judgment', 'Evaluation', 'Final Decision', 'Confidence']\n",
    "\n",
    "\n",
    "# Function to extract continuous score from GPT reply\n",
    "def extract_probabilistic_scores(reply):\n",
    "    parsed = {}\n",
    "    for dim in MP_DIMENSIONS:\n",
    "        key = dim.lower().replace(' ', '_')\n",
    "        match = re.search(rf\"{dim}:\\s*\\[([^\\]]+)\\]\", reply, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                raw_probs = match.group(1).split(\",\")\n",
    "                probs = [float(p.strip()) for p in raw_probs]\n",
    "                if len(probs) == 5 and abs(sum(probs) - 1.0) < 0.1:\n",
    "                    expected = sum(p * i for i, p in enumerate(probs))\n",
    "                    parsed[f\"{key}_continuous\"] = round(expected, 2)\n",
    "                else:\n",
    "                    parsed[f\"{key}_continuous\"] = None\n",
    "            except:\n",
    "                parsed[f\"{key}_continuous\"] = None\n",
    "        else:\n",
    "            parsed[f\"{key}_continuous\"] = None\n",
    "    return parsed\n",
    "\n",
    "\n",
    "# GPT-4 based MP scoring function\n",
    "def gpt_mp_score_continuous_combined(df_subset, model=\"gpt-4-turbo\"):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df_subset.iterrows():\n",
    "        meta = row.to_dict()\n",
    "        text = str(meta[\"combined_reflection\"]).strip()\n",
    "\n",
    "        if not text or len(text) < 10:\n",
    "            auto_filled = {f\"{dim.lower().replace(' ', '_')}_continuous\": 0.0 for dim in MP_DIMENSIONS}\n",
    "            results.append({**meta, **auto_filled, \"gpt_rationale\": \"Auto-scored due to empty or trivial reflection.\"})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "You are a metacognitive scientist evaluating a participant's reflection on a three-choice probabilistic reversal learning task.\n",
    "\n",
    "The following narrative is a response to two reflection questions:\n",
    "1. What strategy did you use?\n",
    "2. Did you switch strategies during the task?\n",
    "\n",
    "Your goal is to evaluate this reflection using the Metacognitive Prompting (MP) framework.\n",
    "\n",
    "---\n",
    "\n",
    "**MP Dimensions (Wang & Zhao, 2025):**\n",
    "\n",
    "You will score the reflection across the following five dimensions:\n",
    "\n",
    "1. **Comprehension** – Did the participant understand the task rules, structure, or goal?  \n",
    "2. **Judgment** – Did they form a hypothesis or plan about how to act?  \n",
    "3. **Evaluation** – Did they reflect on whether their strategy worked, and why?  \n",
    "4. **Final Decision** – Did they describe a consistent rule or decision strategy they ultimately applied?  \n",
    "5. **Confidence** – Did they express certainty or doubt about their approach or understanding?\n",
    "\n",
    "---\n",
    "\n",
    "**Scoring Rubric (used for each dimension):**  \n",
    "Each dimension should be scored on a scale from 0 to 4, where:\n",
    "\n",
    "- **0 = Absent** – No evidence of the dimension  \n",
    "- **1 = Rudimentary** – Vague or minimal expression  \n",
    "- **2 = Adequate** – Some clarity but not in-depth  \n",
    "- **3 = Good** – Clear and thoughtful expression  \n",
    "- **4 = Exemplary** – Sophisticated and comprehensive insight\n",
    "\n",
    "---\n",
    "\n",
    "**Scoring Guidance:**  \n",
    "Please recognize that high-quality metacognition may appear in many forms. Participants may demonstrate deep insight even if they do not explicitly state phrases like “I evaluated” or “I was confident.” You should reward:\n",
    "\n",
    "- Conditional reasoning (e.g., \"if... then...\" statements)  \n",
    "- Hypothetical thinking or counterfactuals  \n",
    "- Thoughtful restraint and uncertainty  \n",
    "- Philosophical or cautious language (e.g., “presumption of innocence”)  \n",
    "- Rational inertia or minimal strategy change, when justified  \n",
    "- Reflective self-awareness (of effort, bias, or internal limits)\n",
    "\n",
    "Short responses can still be insightful. Do not penalize indirect or abstract language if the logic and self-awareness are present.\n",
    "\n",
    "---\n",
    "\n",
    "**Your Task:**  \n",
    "For each dimension, estimate the probability that the reflection deserves a score of 0, 1, 2, 3, or 4.  \n",
    "Respond using decimal values only (e.g., `[0.1, 0.2, 0.3, 0.2, 0.2]`).  \n",
    "The values must:\n",
    "\n",
    "- Be enclosed in square brackets  \n",
    "- Contain exactly 5 decimals  \n",
    "- Sum to 1.0 (±0.01)  \n",
    "- Be purely numeric (no letters)\n",
    "\n",
    "Format your output like this:\n",
    "\n",
    "Comprehension: [0.0, 0.1, 0.2, 0.3, 0.4]  \n",
    "Judgment: [0.2, 0.2, 0.2, 0.2, 0.2]  \n",
    "Evaluation: [0.9, 0.1, 0.0, 0.0, 0.0]  \n",
    "Final Decision: [0.0, 0.0, 0.0, 0.2, 0.8]  \n",
    "Confidence: [0.1, 0.1, 0.1, 0.3, 0.4]  \n",
    "\n",
    "Rationale: Provide a brief explanation for your probability estimates.\n",
    "\n",
    "**Scoring Tips:**\n",
    "- If the reflection shows deep reasoning, nuance, self-awareness, or philosophical restraint, it is okay to assign something like:  \n",
    "  `[0.0, 0.0, 0.0, 0.2, 0.8]`. So, you give this response a 80% probability of scoring closer to a 4 (an exemplary rating) for a particular dimension.  \n",
    "- If the dimension is completely absent, something like:  \n",
    "  `[0.9, 0.1, 0.0, 0.0, 0.0]` is acceptable. So, you give this response a 90% probability of scoring closer to a 0 (an absent rating) for a particular dimension.  \n",
    "- Use the full scale honestly and appropriately. Reflections with strong insight deserve strong scores (> 0.7).\n",
    "\n",
    "---\n",
    "\n",
    "**Reflection:**  \n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content.strip()\n",
    "            parsed_scores = extract_probabilistic_scores(reply)\n",
    "\n",
    "            results.append({\n",
    "                **meta,\n",
    "                **parsed_scores,\n",
    "                \"gpt_rationale\": reply\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on row {idx}: {e}\")\n",
    "            error_filled = {f\"{dim.lower().replace(' ', '_')}_continuous\": None for dim in MP_DIMENSIONS}\n",
    "            results.append({**meta, **error_filled, \"gpt_rationale\": \"error\"})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"hmp_paranoia.csv\")\n",
    "\n",
    "strategy_col = \"Did you use any particular strategy or strategies? If yes, please describe.\"\n",
    "switching_col = \"Did you find yourself switching strategies over the course of the game?\"\n",
    "\n",
    "df['strategy'] = df[strategy_col].fillna(\"\").astype(str).str.strip()\n",
    "df['switching'] = df[switching_col].fillna(\"\").astype(str).str.strip()\n",
    "df['combined_reflection'] = df['strategy'] + \" ; \" + df['switching']\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_to_keep = [\n",
    "    'id', 'mturk_id', 'paranoia_score', 'referential_group', 'persecution_group',\n",
    "    'Did any of the partners deliberately sabotage you?',\n",
    "    'wsr_avg', 'mu02_avg', 'mu03_avg', 'kappa_avg', 'omega2_avg', 'omega3_avg',\n",
    "    'strategy', 'switching', 'combined_reflection'\n",
    "]\n",
    "df_subset = df[cols_to_keep].copy()\n",
    "\n",
    "# Run GPT-MP scoring function\n",
    "df_final = gpt_mp_score_continuous_combined(df_subset)\n",
    "\n",
    "# Reorder final columns\n",
    "final_cols = cols_to_keep + [f\"{dim.lower().replace(' ', '_')}_continuous\" for dim in MP_DIMENSIONS] + ['gpt_rationale']\n",
    "df_final = df_final[[col for col in final_cols if col in df_final.columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id        mturk_id  paranoia_score referential_group persecution_group  \\\n",
      "0   1  A2541C8MY0BYV3        0.000000               low               low   \n",
      "1   2  A3JSDZMBS8L87S        0.722222               low               low   \n",
      "2   3  A1PR74OHURJNTO        0.250000               low               low   \n",
      "3   4   A1CSDIX05PK9V        0.000000               low               low   \n",
      "4   5  A1JJYY622DGE5L        0.444444               low               low   \n",
      "\n",
      "   Did any of the partners deliberately sabotage you?   wsr_avg  mu02_avg  \\\n",
      "0                                               -2.0   0.000000 -0.134659   \n",
      "1                                               -2.0   0.017857 -0.326276   \n",
      "2                                               -3.0   0.000000 -0.011850   \n",
      "3                                               -2.0   0.010000 -0.129578   \n",
      "4                                               -2.0   0.000000 -0.914451   \n",
      "\n",
      "   mu03_avg  kappa_avg  ...  omega3_avg  \\\n",
      "0 -0.035434   0.428227  ...   -0.923158   \n",
      "1 -0.491824   0.455290  ...   -0.568286   \n",
      "2  0.992294   0.602294  ...   -1.995328   \n",
      "3 -0.744701   0.446320  ...    0.213838   \n",
      "4 -3.778985   0.431306  ...   -1.664934   \n",
      "\n",
      "                                            strategy  \\\n",
      "0  I would stick with a partner until they receiv...   \n",
      "1                                                No.   \n",
      "2  If I was losing points more than half the time...   \n",
      "3                                                 no   \n",
      "4  I tried to select the same person as long as i...   \n",
      "\n",
      "                                           switching  \\\n",
      "0  I did. At first I found a pattern, but that pa...   \n",
      "1                                                No.   \n",
      "2                                        Not really.   \n",
      "3                                          yes i did   \n",
      "4                                                      \n",
      "\n",
      "                                 combined_reflection comprehension_continuous  \\\n",
      "0  I would stick with a partner until they receiv...                      3.4   \n",
      "1                                          No. ; No.                      0.0   \n",
      "2  If I was losing points more than half the time...                      2.5   \n",
      "3                                     no ; yes i did                      0.1   \n",
      "4  I tried to select the same person as long as i...                      2.5   \n",
      "\n",
      "   judgment_continuous  evaluation_continuous  final_decision_continuous  \\\n",
      "0                  3.5                    3.3                        3.4   \n",
      "1                  0.0                    0.0                        0.0   \n",
      "2                  3.1                    2.9                        3.4   \n",
      "3                  0.1                    0.1                        0.1   \n",
      "4                  3.1                    1.7                        3.0   \n",
      "\n",
      "   confidence_continuous                                      gpt_rationale  \n",
      "0                    3.0  Comprehension: [0.0, 0.0, 0.1, 0.4, 0.5]\\nJudg...  \n",
      "1                    0.0    Auto-scored due to empty or trivial reflection.  \n",
      "2                    2.0  Comprehension: [0.0, 0.1, 0.4, 0.4, 0.1]\\nJudg...  \n",
      "3                    0.1  Comprehension: [0.9, 0.1, 0.0, 0.0, 0.0]  \\nJu...  \n",
      "4                    1.6  Comprehension: [0.0, 0.1, 0.4, 0.4, 0.1]  \\nJu...  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_final.to_csv(\"mp_scores_continuous_500.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
